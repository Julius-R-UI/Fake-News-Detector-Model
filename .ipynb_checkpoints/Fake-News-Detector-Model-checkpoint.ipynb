{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/julius_riel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "import keras\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.layers import Dropout, Input,Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from transformers import *\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, DistilBertConfig,DistilBertModel\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "DistiledBertTokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "DistiledBert = TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = pd.read_csv('True.csv')\n",
    "fake = pd.read_csv('Fake.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatonation of data in real.csv and fake.csv, with addition of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake[\"labels\"] = 1\n",
    "real[\"labels\"] = 0\n",
    "\n",
    "Dataset = [fake,real]\n",
    "Dataset = pd.concat(Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting of data into features (x) and labels (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dataset.drop(\"labels\", axis = 1)\n",
    "y = Dataset.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding of title to the body of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"text\"] = x[\"title\"] + \" \" + x[\"text\"]\n",
    "x = x.drop(\"title\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal of HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(string):\n",
    "    result = BeautifulSoup(string, \"html.parser\")#re.sub('<.*?>','',string)\n",
    "    result = result.get_text()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal of special characters & punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(string):\n",
    "    result = re.sub('\\[[^]]*\\]', ' ', string)\n",
    "    result = re.sub('[^a-zA-Z]',' ', string)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string,max_length):\n",
    "    bert_inp=DistiledBertTokenizer.encode_plus(string,add_special_tokens = True,max_length =max_length,pad_to_max_length = True,return_attention_mask = True)\n",
    "    input_ids = (bert_inp['input_ids'])\n",
    "    attentionmasks = (bert_inp['attention_mask'])\n",
    "    \n",
    "    return input_ids, attentionmasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal of Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered = []\n",
    "    for i in tokenized_text:\n",
    "        if i not in stop_words:\n",
    "            filtered.append(i)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Data preperation proccesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "x[\"text\"] = x[\"text\"].apply(lambda x: \"'\" + x + \"'\")\n",
    "x[\"text\"] = x['text'].str.lower()\n",
    "x['text'] = x['text'].apply(lambda f : remove_tags(f))\n",
    "x['text'] = x['text'].apply(lambda f : remove_special_characters(f))\n",
    "#x['text'] = x.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "#x['text'] = x['text'].apply(lambda f: remove_stopwords(f))\n",
    "input_id = []\n",
    "attention_mask = []\n",
    "max_length = 512\n",
    "for i in x['text']:\n",
    "    inpt, attention = tokenize(i,max_length)\n",
    "    input_id.append(inpt)\n",
    "    attention_mask.append(attention)\n",
    "\n",
    "input_id = np.asarray(input_id)\n",
    "attention_mask = np.array(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_train,id_test,y_train,y_test,attentionmask_train,attentionmask_test=train_test_split(input_id,y,attention_mask,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a simple Neural Network to use the outputted to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "ind = Input(shape = (max_length,), dtype='int64')\n",
    "attmasks = Input(shape = (max_length,), dtype='int64')\n",
    "number_of_classes = 2\n",
    "\n",
    "DistiledBERTLayer = DistiledBert(ind, attention_mask=attmasks)[0][:,0,:]\n",
    "dense_layer = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(DistiledBERTLayer)\n",
    "dropout_layer= Dropout(0.5)(dense_layer)\n",
    "Output = Dense(number_of_classes, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout_layer)\n",
    "model = tf.keras.Model(inputs=[ind,attmasks], outputs=Output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011/1011 [==============================] - ETA: 0s - loss: 3.6467 - accuracy: 0.9969 WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1011/1011 [==============================] - 26937s 27s/step - loss: 3.6467 - accuracy: 0.9969 - val_loss: 1.8405 - val_accuracy: 0.9992\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit([id_train,attentionmask_train],y_train,batch_size=32,epochs=1, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([id_test, attentionmask_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
